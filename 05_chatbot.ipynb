{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c2bef1a",
   "metadata": {},
   "source": [
    "### ChatBot Assistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bf30cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CIcsv4JoqjSYtSWXVTBE6wqmBAabc', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--9199082a-3747-4047-866e-29f6f21ec517-0', usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] == os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "llm.invoke(\"Hi\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0048226f",
   "metadata": {},
   "source": [
    "### Agentic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcef4e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agentic RAG\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.tools.retriever import create_retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b5a2b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-07-22T15:47:11+05:30', 'title': 'Sagar Parab', 'author': 'Dell', 'moddate': '2025-07-22T15:47:11+05:30', 'source': 'C:\\\\Users\\\\Dell\\\\OneDrive\\\\Documents\\\\Sagar_Parab_Resume.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}, page_content='Sagar Parab \\n \\n \\nContact \\n  \\nAddress  \\nDombivali (Thane), India 421202 \\nPhone  \\n+919167104607 \\nE-mail  \\nsagarparab1987@gmail.com \\nWWW \\nhttps://www.linkedin.com/in/sagar\\n-parab-23290a1a8/ \\n  \\nSkills \\n  \\n• AI / DATA SCIENCE: \\nPython, Pandas, \\nNumPy, Scikit-learn, \\nTensorFlow, Keras, \\nPyTorch, BERT, \\nHugging Face \\nTransformers, LSTM, \\nCNN, RNN, Prompt \\nEngineering, \\nLangChain, Agents, \\nRAG, Flask, Streamlit, \\nDocker, AWS (ECR, \\nECS)  \\n \\n \\n• DATA ANALYTICS & BI \\nTOOLS: MS Excel \\n(Advanced), VBA \\nMacros, Power BI, \\nTableau, Power \\nQuery, Power \\nAutomate, SSRS, SSIS, \\nSQL  \\n \\n \\n• MLOps & \\nDEPLOYMENT: \\nDocker, Flask,  \\n \\nDynamic and results -driven professional with 14+ years of experience in analytics, \\nautomation, and reporting, now strategically transitioning into the AI and Data \\nScience industry. Successfully completing a rigorous AI/Data Science & Generative \\nAI program at Scaler Academy (July 2025), with comprehensive hands -on \\nexperience in machine learning, deep learning, and Gen -AI model deployment. \\nProven track record of delivering significant business impact using advanced Excel, \\nVBA, Power BI, and Power Query. Eager to contribute to innovative, AI -driven \\nsolutions in a forward -thinking organization.  \\n  \\nPROFESSIONAL EXPERIENCE \\n  \\n  2022-03 - \\n2024-09 \\n  Senior Analyst  \\nAccenture Services   \\n• Designed and built comprehensive dashboards in Power \\nBI, providing critical business insights.  \\n• Developed automated Excel reports using VBA & Power \\nQuery, significantly enhancing reporting efficiency.  \\n• Delivered impactful analytics and presentations to senior \\nstakeholders, influencing data -driven decisions.  \\n• Partnered effectively with stakeholders for SLA/KPI \\ntracking and improvements, ensuring operational \\nexcellence.  \\n• Standardized and streamlined reporting processes with a \\nstrong focus on data integrity and accuracy.  \\n \\n  2014-06 - \\n2021-12 \\n  Senior Analyst  \\nWillis Towers Watson   \\n• Led reporting and dashboarding initiatives utilizing SQL, \\nPower BI, and Excel VBA.  \\n• Automated numerous manual reporting processes, \\nresulting in an estimated 30% time savings.  \\n• Collaborated cross -functionally to drive significant \\nreporting enhancements and system improvements.  \\n  \\n  2009-08 - \\n2014-05 \\n  Team Coordinator  \\nBank of America   \\n• Designed and implemented productivity and quality \\nreports for home loan operations.  \\n• Led VBA (crawling scripts) automation initiatives to \\nimprove operational efficiency and data collection.  \\n• Trained and managed new team members across 5+ \\nloan processes, ensuring high performance.'), Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-07-22T15:47:11+05:30', 'title': 'Sagar Parab', 'author': 'Dell', 'moddate': '2025-07-22T15:47:11+05:30', 'source': 'C:\\\\Users\\\\Dell\\\\OneDrive\\\\Documents\\\\Sagar_Parab_Resume.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}, page_content='Streamlit, Git, AWS, \\nREST APIs, ECR, ECS, \\nModel Deployment \\n \\n• SOFT SKILLS: \\nStakeholder \\nCommunication, \\nTeam Leadership, \\nAnalytical Thinking, \\nProcess Optimization, \\nProject Ownership  \\n \\n \\n  \\nLANGUAGES \\n    \\nEnglish \\n \\n \\n  \\nMarathi \\n \\n \\n  \\nHindi \\n \\n  \\nWebsites, Portfolios, \\nProfiles \\n  \\n• https://www.linkedin.com/in/sa\\ngar-parab-23290a1a8/  \\n  \\nEDUCATION \\n  \\n  2011-04   M.Com from University of Mumbai   \\n  2008-03   B.Com from University of Mumbai   \\n  \\nPROJECT EXPERIENCE \\n  \\nGen AI – AgenticAI_Research_Tool_Project  \\nIdeating| Resource_Collection| Streamlit  \\n• AI-powered companion for research assistance using \\nYouTube Search, Semantic Scholar, and Playwright.  \\n• Ideating page for expanding ideas and identifying \\nresearch gaps.  \\n• Research Collection page to gather and organize \\nacademic materials.  \\n \\nGen AI - Open AI_Project  \\ngrammar_fun| reading_translation | image_comprehension | \\nStreamlit  \\n• Developed an interactive AI -powered English learning \\nplatform using Streamlit and OpenAI APIs.  \\n• Integrated grammar quizzes, image -based speaking \\ntasks with speech -to-text via Whisper, and Hindi -to-\\nEnglish translation evaluation.  \\n• Enabled real -time feedback through GPT models and \\nvoice input using SoundDevice, enhancing user \\nengagement through multimodal language practice.  \\n \\nSentiment Analysis & Translation App  \\nBERT + MarianMT | Transformers | Flask | GPU Deployment  \\n• Developed and trained a BERT -based model on the \\nAmazon Polarity dataset for highly accurate sentiment \\nclassification.  \\n• Integrated MarianMT for English -to-Hindi translation, \\ndemonstrating proficiency in NLP and language \\nmodels.  \\n• Designed and implemented a user -friendly Streamlit \\nfrontend to display text, translation, and sentiment \\noutput.'), Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-07-22T15:47:11+05:30', 'title': 'Sagar Parab', 'author': 'Dell', 'moddate': '2025-07-22T15:47:11+05:30', 'source': 'C:\\\\Users\\\\Dell\\\\OneDrive\\\\Documents\\\\Sagar_Parab_Resume.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3'}, page_content=\"Spam Detection using BERT  \\nDeep Learning | Hugging Face | Flask  \\n• Fine-tuned a 'bert -base-uncased' model for robust \\nspam classification, achieving a 96% F1 -score on the \\nvalidation set.  \\n• Deployed the model as a scalable web application \\nusing Flask, showcasing end -to-end model deployment \\ncapabilities.  \\n \\nTime Prediction Portal App  \\nNeural Networks | Flask | Target Encoding  \\n• Built a time prediction model using a custom neural \\nnetwork architecture.  \\n• Applied Mean Squared Error (MSE) as the loss function \\nand incorporated target encoding for improved model \\nperformance.  \\n• Deployed the predictive model via Flask, ensuring \\naccessibility and usability.  \\n \\nInsurance Cost Prediction  \\nML Models: GBDT, XGBoost, Linear, KNN | EDA | Flask App  \\n• Achieved 91% accuracy using a Gradient Boosting \\nDecision Tree (GBDT) model for insurance cost \\nprediction.  \\n• Conducted extensive feature engineering, outlier \\nremoval, and hypothesis testing to optimize data \\nquality and model inputs.  \\n• Developed and deployed a Flask web application for \\ninteractive predictions.  \\n \\nProduct Sales Forecasting  \\nStacked Models | Flask | Docker | Time Series (SARIMAX)  \\n• Developed hybrid stacked models for product sales \\nforecasting, achieving 96% accuracy.  \\n• Utilized SARIMAX for time series analysis, demonstrating \\nexpertise in advanced forecasting techniques.  \\n• Deployed the application using Flask and Docker for \\nscalable and efficient access.  \\n  \\nWEBSITES, PORTFOLIOS, PROFILES \\n  \\nhttps://www.linkedin.com/in/sagar -parab-23290a1a8/  \\n. #HRJ#1dc3 0251-fb3f-4fb0-8990-f88b2a51 55a2#\")]]\n"
     ]
    }
   ],
   "source": [
    "pdf_files_1 = [r\"C:\\Users\\Dell\\OneDrive\\Documents\\Sagar_Parab_Resume.pdf\"]\n",
    "             \n",
    "\n",
    "docs_1 = [PyPDFLoader(doc).load() for doc in pdf_files_1]\n",
    "print(docs_1)\n",
    "\n",
    "docs_list_1 = [item for sublist in docs_1 for item in sublist]\n",
    "\n",
    "text_splitter_1 = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "doc_splits_1 = text_splitter_1.split_documents(docs_list_1)\n",
    "## add all these text to vectordb\n",
    "vectorstore_1 = FAISS.from_documents(\n",
    "    documents=doc_splits_1,\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever_1 = vectorstore_1.as_retriever()\n",
    "\n",
    "### retriver tool\n",
    "retriever_resume_tool_1 = create_retriever_tool(\n",
    "    retriever_1,\n",
    "    \"retriever_vector_db_blog\",\n",
    "    \"search and find the information about Sagar resume\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0b0a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Document(metadata={'producer': 'Skia/PDF m134', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36', 'creationdate': '2025-05-27T16:14:52+00:00', 'title': '11-BERT.ipynb - Colab', 'moddate': '2025-05-27T16:14:52+00:00', 'source': 'C:\\\\Users\\\\Dell\\\\OneDrive\\\\Documents\\\\BERT summary.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1'}, page_content='Introduction to Business Case: Data Scientist at Medical Research Publication company\\nIntroduction to BERT: Why is it SOTA?\\nDifferences between RNN and BERT\\nBERT: Embedding vs Pretrained model\\nHow Bert is Pretrained\\nMasked Language Modelling\\nNext Sentence Prediction\\nMLM vs NSP\\nBidirectionality of BERT\\nShort recap of Transformers\\nAttention mechanisms\\nPositional encoding\\nTokenization in BERT\\nWord Piece Tokenizer\\nTypes and Applications of BERT\\nTypes of BERT\\nApplication Classes in huggingface for BERT\\nContent\\nYou are a Data scientist working in a Medical Research publication company.\\nAs 100s of articles are published every single day, the organizers wants to identify which\\ndiseases are being spoke in the article.\\nManually reading the articles and identifying the disease is a laborious process.\\nAs a data scientist you are asked, if you can train a machine Learning model to automatically\\nidentify the diseases mentioned in the corpus with the limited training data available\\nBusiness Case\\ue313\\nWe can use NER models available in Spacy or train RNN models, etc\\nTo solve this problem we will be training a NER model using BERT.\\n5/27/25, 9:44 PM 11-BERT.ipynb - Colab\\nhttps://colab.research.google.com/drive/1OHrWcsDrxc5tAKjpSy4uvbS83p8EA-56?usp=drive_link#scrollTo=OeACWngIKwKJ&printMode=true 1/8'), Document(metadata={'producer': 'Skia/PDF m134', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36', 'creationdate': '2025-05-27T16:14:52+00:00', 'title': '11-BERT.ipynb - Colab', 'moddate': '2025-05-27T16:14:52+00:00', 'source': 'C:\\\\Users\\\\Dell\\\\OneDrive\\\\Documents\\\\BERT summary.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2'}, page_content=\"Since BERT has been the State Of The Art in the almost all NLP task in the recent years, we\\nwill use it to train with our corpus\\nBERT has been outperforming in almost all benchmarking NLP task like Question Answering,\\nNext Sentence Prediction tasks etc\\nBecause of the way it was pretrained with rich corpus of data it was able to achieve SOTA\\nresults.\\nWe will see the details in the subsequent sections\\nBut why is BERT the SOTA and how was it able to achieve?\\ue313\\nBERT stands for BiDirectional Encoder Representation from Transformers released by Google\\nResearch team in 2018\\nBERT uses only the Encoder portion of the transformer's architecture which is the reason it's\\ncalled Encoder Representation from transformers.\\nLet us begin with what is BERT?\\ue313\\n5/27/25, 9:44 PM 11-BERT.ipynb - Colab\\nhttps://colab.research.google.com/drive/1OHrWcsDrxc5tAKjpSy4uvbS83p8EA-56?usp=drive_link#scrollTo=OeACWngIKwKJ&printMode=true 2/8\"), Document(metadata={'producer': 'Skia/PDF m134', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36', 'creationdate': '2025-05-27T16:14:52+00:00', 'title': '11-BERT.ipynb - Colab', 'moddate': '2025-05-27T16:14:52+00:00', 'source': 'C:\\\\Users\\\\Dell\\\\OneDrive\\\\Documents\\\\BERT summary.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3'}, page_content=\"RNN BERT\\nIt is built with recurrent blocks It is built with transformer blocks\\nEach word in a sequence arepassed one after the other The entire sentence is passed at once\\nIf N is the no.of words in a sequence,RNNs has to compute N sequential steps It performs only one step forthe entire sequence of length N\\nSimilarly for the gradients to propagatefrom last word to the first word it takes N stepsIn Bert, it is just one step process\\nBecause of sequential processingthere is no parallelization It can be parallelization sinceit is not passed sequentially\\nThey suffer from Vanishing gradientsthough LSTM/GRU can handle to an extent Bert does not suffer from Vanishing gradientproblem and can learn long term dependencies very well\\nNo self attention Has self attenion\\nHow is BERT different from other architectures like RNN(LSTM/GRU)?\\nThough we might have learnt it before. Let's understand the difference:\\nEmbedding model:\\nIn Embedding models you only get the embeddings for each word or sentence of specified\\ndimensions\\nWith the embeddings, you will have to fit a ML model like Logistic Regression, Random\\nForest, XGboost etc or even NN using the embeddings as features.\\nExample: Word2Vec, Glove, ELmo etc\\nIs BERT a word embedding model or a pretrained model?\\n5/27/25, 9:44 PM 11-BERT.ipynb - Colab\\nhttps://colab.research.google.com/drive/1OHrWcsDrxc5tAKjpSy4uvbS83p8EA-56?usp=drive_link#scrollTo=OeACWngIKwKJ&printMode=true 3/8\"), Document(metadata={'producer': 'Skia/PDF m134', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36', 'creationdate': '2025-05-27T16:14:52+00:00', 'title': '11-BERT.ipynb - Colab', 'moddate': '2025-05-27T16:14:52+00:00', 'source': 'C:\\\\Users\\\\Dell\\\\OneDrive\\\\Documents\\\\BERT summary.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4'}, page_content=\"Pretrained model:\\nIf you are familiar with transfer learning/pretrained models in computer vision like AlexNet,\\nVGGNet, the same concept is used here where the architecure of the pre-trained models with\\nweights are fixed.\\nYou can just add the output layer with different number of neurons based on your task.\\nSo now is BERT a word embedding model or a pretrained model?\\nActually it is both.\\n * Just like other word embedding models like Word2Vec, ELmo, Glove etc you can extract\\nthe word embedding for each word/sentences.\\n * This can be achieved by accessing the embedding layer before it is passed to the Encoder\\nportion of the architecture.\\n * You can also use the BERT architecure with random weights or pretrained weights to train\\na model just like pretrained models in computer vision by transfer learning.\\nCan we use Random weights instead of Pretrained weights?\\nIn >90% of the cases we use the pretrained weights since these weights are trained weights\\nare trained on general purpose corpus, it's easy to optimize.\\nIf we start with random weights, It can even take days/weeks to get good results compared to\\nstarting from pre-trained weights\\nBut is there any usecase we can start with Random weights?\\nYes it can be useful in some cases where you are trying to pre-train a BERT model from\\nscratch specific to a domain. Example:\\nBIO-BERT: It is pretrained on the corpus specific to Medical domain.\\nLawBERT: It is pretrained on the corpus specific to Legal domain.\\nOnce pretrained these weights can be used for task specific to that domain instead of starting from\\nthe native BERT embeddings.\\nBert was trained on two tasked\\n1. Predicting the masked words\\n2. Next Sentence Prediction\\nWhat was the data that it was pretrained on?\\nHow was BERT pre-trained?\\ue313\\n5/27/25, 9:44 PM 11-BERT.ipynb - Colab\\nhttps://colab.research.google.com/drive/1OHrWcsDrxc5tAKjpSy4uvbS83p8EA-56?usp=drive_link#scrollTo=OeACWngIKwKJ&printMode=true 4/8\"), Document(metadata={'producer': 'Skia/PDF m134', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36', 'creationdate': '2025-05-27T16:14:52+00:00', 'title': '11-BERT.ipynb - Colab', 'moddate': '2025-05-27T16:14:52+00:00', 'source': 'C:\\\\Users\\\\Dell\\\\OneDrive\\\\Documents\\\\BERT summary.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5'}, page_content='It was Pretained on Wikipedia(2500 million words) and Book Corpus(800 million words)\\nMasking 15% of the words and predict them\\n * By Randomly masking 15% of the words in the corpus the, model was trained to predict\\nthe masked words.\\nExample:\\nActual sentence Masked train sentence Label\\nToday morning, I went fora tooth removal to my dentist Today morning, I went for a< MASK > removal to my dentist tooth\\nDid you go to school today Did you < MASK > to school todaygo\\n1. Predicting the masked words\\nPredicting if the next sentence is continuation of the previous sentence or not\\nExample:\\nSentence A Sentence B IsNext?\\nThe Indian cricket team wonthe match against West Indies. Obama served 2 terms asthe president of USA. No\\nNadal won the 2022 Australian open. With this victory he now has 21GrandSlam titles to his name. Yes\\nBut why do we need a NSP and why not just MLM alone?\\nIn masked language modeling (MLM), the model is learning the contextual information well.\\nBut for someother downstream tasks like QnA, text generation etc, MLM was not capturing\\nthe relations retionship between sentences.\\nBy pretraining with the MLM and NSP together, they were able to achieve good performance\\non QnA and Natural Language Inference (NLI) tasks\\nTraining data and label generation\\nThe data was generated in such a way that 50% of the time the Sentence B is a continuation\\nof Sentence A and labeled as True for < IsNext >\\nIn the rest 50%, Sentence B is randomly sampled which is not a continuation of Sentence A\\nand labeled as False for < IsNext >\\nThe combined length of the two chosen sentences was <= 512 tokens.\\nOnce each sequence was built, 15% of its tokens were masked for the MLM task\\n2. Next Sentence Prediction\\ue313\\n5/27/25, 9:44 PM 11-BERT.ipynb - Colab\\nhttps://colab.research.google.com/drive/1OHrWcsDrxc5tAKjpSy4uvbS83p8EA-56?usp=drive_link#scrollTo=OeACWngIKwKJ&printMode=true 5/8'), Document(metadata={'producer': 'Skia/PDF m134', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36', 'creationdate': '2025-05-27T16:14:52+00:00', 'title': '11-BERT.ipynb - Colab', 'moddate': '2025-05-27T16:14:52+00:00', 'source': 'C:\\\\Users\\\\Dell\\\\OneDrive\\\\Documents\\\\BERT summary.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6'}, page_content='As we saw BERT is pretrained on two tasks MLM and NSP together, the losses from both the\\ntasks are summed together to get the final loss.\\nHow is the loss calculated?\\ue313\\nStart coding or generate  with AI.\\nStart coding or generate  with AI.\\nStart coding or generate  with AI.\\nStart coding or generate  with AI.\\nStart coding or generate  with AI.\\nStart coding or generate  with AI.\\nStart coding or generate  with AI.\\nStart coding or generate  with AI.\\nStart coding or generate  with AI.\\nStart coding or generate  with AI.\\nBut the training data is not labelled, What type of learning is it?\\n5/27/25, 9:44 PM 11-BERT.ipynb - Colab\\nhttps://colab.research.google.com/drive/1OHrWcsDrxc5tAKjpSy4uvbS83p8EA-56?usp=drive_link#scrollTo=OeACWngIKwKJ&printMode=true 6/8'), Document(metadata={'producer': 'Skia/PDF m134', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36', 'creationdate': '2025-05-27T16:14:52+00:00', 'title': '11-BERT.ipynb - Colab', 'moddate': '2025-05-27T16:14:52+00:00', 'source': 'C:\\\\Users\\\\Dell\\\\OneDrive\\\\Documents\\\\BERT summary.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7'}, page_content=\"This method of training is called semi-supervised learning method.\\n * Semi-supervised training sits between supervised and unsupervised training.\\n * In this method we don't have labels like supervised learning but we try to create labels\\nfrom the data available and not manually labelling them.\\nToday morning, I went for a ___\\nToday morning, I went for a ___ removal to my dentist\\nIn the example above it is easier to predict in the sencond instance comapred to 1st, since we can\\nsee what are the words coming after the ___.\\nWhat is Bidirectional about Bert?\\nHas access to the words to the left and right of the masked word.\\n * When training the model to predict the masked word it has access to the words to the left\\nand the right side of the masked word.\\n * This method helps in understanding the context of the language better unlike having\\naccess only to the words from left of the masked words.\\n * For this reason BERT is called a Masked Language Model, whereas other architecures like\\nGPT are called Language model since they have access to words only from the left to make\\nthe prediction of the next word.\\nWhich of the 2 is it easier to predict?\\ue313\\n5/27/25, 9:44 PM 11-BERT.ipynb - Colab\\nhttps://colab.research.google.com/drive/1OHrWcsDrxc5tAKjpSy4uvbS83p8EA-56?usp=drive_link#scrollTo=OeACWngIKwKJ&printMode=true 7/8\"), Document(metadata={'producer': 'Skia/PDF m134', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36', 'creationdate': '2025-05-27T16:14:52+00:00', 'title': '11-BERT.ipynb - Colab', 'moddate': '2025-05-27T16:14:52+00:00', 'source': 'C:\\\\Users\\\\Dell\\\\OneDrive\\\\Documents\\\\BERT summary.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8'}, page_content=\"Word Piece Tokenizer method\\nBert uses a special tokenization mechanism called the WordPiece Tokenizer where\\ncompound words are split into root word and subwords separated with ##.\\nThe word-based tokenizer suffers from very large vocabulary, large number of OOV\\ntokens which is addressed by Word Piece Tokenizer\\nLet's look at a example\\nHow are the words tokenized in Bert?\\ue313\\nfrom transformers import BertTokenizer\\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\\nwords = ['parachute', 'paraglide', 'paragliding', 'scubadiving', 'scubadiver', 'scubadive']\\nfor word in words:\\n    tok_word = tokenizer.tokenize(word)\\n    print(f'{word:<12} tokenized to {tok_word}')\\nparachute    tokenized to ['parachute']\\nparaglide    tokenized to ['para', '##gli', '##de']\\nparagliding  tokenized to ['para', '##gli', '##ding']\\nscubadiving  tokenized to ['scuba', '##di', '##ving']\\nscubadiver   tokenized to ['scuba', '##di', '##ver']\\nscubadive    tokenized to ['scuba', '##di', '##ve']\\n5/27/25, 9:44 PM 11-BERT.ipynb - Colab\\nhttps://colab.research.google.com/drive/1OHrWcsDrxc5tAKjpSy4uvbS83p8EA-56?usp=drive_link#scrollTo=OeACWngIKwKJ&printMode=true 8/8\")]]\n"
     ]
    }
   ],
   "source": [
    "pdf_files_2 = [r\"C:\\Users\\Dell\\OneDrive\\Documents\\BERT summary.pdf\"]\n",
    "\n",
    "docs_2 = [PyPDFLoader(doc).load() for doc in pdf_files_2]\n",
    "print(docs_2)\n",
    "\n",
    "docs_list_2 = [item for sublist in docs_2 for item in sublist]\n",
    "\n",
    "text_splitter_2 = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=50)\n",
    "\n",
    "doc_splits_2 = text_splitter_2.split_documents(docs_list_2)\n",
    "## add all these text to vectordb\n",
    "vectorstore_2 = FAISS.from_documents(\n",
    "    documents=doc_splits_2,\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever_2 = vectorstore_2.as_retriever()\n",
    "\n",
    "### retriver tool\n",
    "retriever_resume_tool_2 = create_retriever_tool(\n",
    "    retriever_2,\n",
    "    \"retriever_vector_db1_blog\",\n",
    "    \"search and find the information about BERT\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "989ac04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "# from langchain_core.messages import BaseMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "\n",
    "# create State class\n",
    "class State(TypedDict):\n",
    "    messages:Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d83b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- built up tools ---\n",
    "def add(a:int, b:int)-> int:\n",
    "    \"\"\" Add a and b\n",
    "    Args:\n",
    "        a (int): first int\n",
    "        b (int): second int\n",
    "\n",
    "    Returns:\n",
    "        int\n",
    "    \"\"\"\n",
    "    return a+b\n",
    "\n",
    "\n",
    "def substract(a:int, b:int)-> int:\n",
    "    \"\"\" Substract a and b\n",
    "    Args:\n",
    "        a (int): first int\n",
    "        b (int): second int\n",
    "\n",
    "    Returns:\n",
    "        int\n",
    "    \"\"\"\n",
    "    return a-b\n",
    "\n",
    "def multiply(a:int, b:int)-> int:\n",
    "    \"\"\" Multiply a and b\n",
    "    Args:\n",
    "        a (int): first int\n",
    "        b (int): second int\n",
    "\n",
    "    Returns:\n",
    "        int\n",
    "    \"\"\"\n",
    "    return a*b\n",
    "\n",
    "def divide(a:int, b:int)-> int:\n",
    "    \"\"\" Divide a and b\n",
    "    Args:\n",
    "        a (int): first int\n",
    "        b (int): second int\n",
    "\n",
    "    Returns:\n",
    "        int\n",
    "    \"\"\"\n",
    "    return a/b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81f56fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [add, substract, multiply, divide, retriever_resume_tool_1, retriever_resume_tool_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3be053a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_agent(state: State):\n",
    "    \"\"\"\n",
    "    Invokes this model to generate the response based on the current state given.\n",
    "    Given the question, model will decide to retrieve using the retriever tool, or simply end.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "    Returns:\n",
    "        dict: The updated state with the agent response appended to messages\n",
    "    \"\"\"\n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state[\"messages\"]\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "    respose = llm_with_tools.invoke(messages)\n",
    "\n",
    "    # we will return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [respose]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9ceed28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State)\n",
    "\n",
    "### node\n",
    "graph_builder.add_node(\"llm_agent\", llm_agent)\n",
    "graph_builder.add_node(\"retrieve\", ToolNode(tools))\n",
    "\n",
    "\n",
    "graph_builder.add_edge(START, \"llm_agent\")\n",
    "graph_builder.add_conditional_edges(\"llm_agent\", tools_condition, {\"tools\":\"retrieve\", END:END})\n",
    "graph_builder.add_edge(\"llm_agent\", END)\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "### Display\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# graph_image = graph.get_graph().draw_mermaid_png()\n",
    "# display(Image(graph_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "699e9776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hi, My name is Sagar and I love to play Cricket', additional_kwargs={}, response_metadata={}, id='45d8a5c8-c5df-45b7-8dab-d2b13be49110'),\n",
       "  AIMessage(content=\"Hi Sagar! It's great to meet you. Cricket is a fantastic sport. Do you play it often, or do you have a favorite team or player?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 269, 'total_tokens': 302, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CIcyYQYFT2dExNcopFXUfcRN6OpRj', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--ffb3e701-71e4-4c95-8a35-d490ec91427c-0', usage_metadata={'input_tokens': 269, 'output_tokens': 33, 'total_tokens': 302, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invocation input\n",
    "graph.invoke({\"messages\": \"Hi, My name is Sagar and I love to play Cricket\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08f52d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n",
      "{'llm_agent': {'messages': [AIMessage(content=\"Hello Sagar! It's great to meet you. Cricket is a fantastic sport. Do you play it often, or do you have a favorite team or player?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 268, 'total_tokens': 301, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CIcyg2dLgL68XoZpamXD6NFXbtqix', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--8151aaf1-63a5-4ca6-8f4a-cc25ace6e199-0', usage_metadata={'input_tokens': 268, 'output_tokens': 33, 'total_tokens': 301, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n"
     ]
    }
   ],
   "source": [
    "for event in graph.stream({\"messages\":\"Hello My name is Sagar and I love to play cricket\"}):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b629cd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is 2 + 2\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  add (call_Cj7uTEOgyeZqHF0F6GyQjQCj)\n",
      " Call ID: call_Cj7uTEOgyeZqHF0F6GyQjQCj\n",
      "  Args:\n",
      "    a: 2\n",
      "    b: 2\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: add\n",
      "\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "messages =  graph.invoke({\"messages\":\"What is 2 + 2\"})\n",
    "\n",
    "for message in messages[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "09e4f9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is qualification of Sagar and what is 2 * 9\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retriever_vector_db_blog (call_nx37bVuHDFFcoLVasajPTJx4)\n",
      " Call ID: call_nx37bVuHDFFcoLVasajPTJx4\n",
      "  Args:\n",
      "    query: Sagar qualification\n",
      "  multiply (call_OihmQK5Tmv48I1x0fQGneUuF)\n",
      " Call ID: call_OihmQK5Tmv48I1x0fQGneUuF\n",
      "  Args:\n",
      "    a: 2\n",
      "    b: 9\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retriever_vector_db_blog\n",
      "\n",
      "Sagar Parab \n",
      " \n",
      " \n",
      "Contact \n",
      "  \n",
      "Address  \n",
      "Dombivali (Thane), India 421202 \n",
      "Phone  \n",
      "+919167104607 \n",
      "E-mail  \n",
      "sagarparab1987@gmail.com \n",
      "WWW \n",
      "https://www.linkedin.com/in/sagar\n",
      "-parab-23290a1a8/ \n",
      "  \n",
      "Skills \n",
      "  \n",
      "• AI / DATA SCIENCE: \n",
      "Python, Pandas, \n",
      "NumPy, Scikit-learn, \n",
      "TensorFlow, Keras, \n",
      "PyTorch, BERT, \n",
      "Hugging Face \n",
      "Transformers, LSTM, \n",
      "CNN, RNN, Prompt \n",
      "Engineering, \n",
      "LangChain, Agents, \n",
      "RAG, Flask, Streamlit, \n",
      "Docker, AWS (ECR, \n",
      "ECS)  \n",
      " \n",
      " \n",
      "• DATA ANALYTICS & BI \n",
      "TOOLS: MS Excel \n",
      "(Advanced), VBA\n",
      "\n",
      "Streamlit, Git, AWS, \n",
      "REST APIs, ECR, ECS, \n",
      "Model Deployment \n",
      " \n",
      "• SOFT SKILLS: \n",
      "Stakeholder \n",
      "Communication, \n",
      "Team Leadership, \n",
      "Analytical Thinking, \n",
      "Process Optimization, \n",
      "Project Ownership  \n",
      " \n",
      " \n",
      "  \n",
      "LANGUAGES \n",
      "    \n",
      "English \n",
      " \n",
      " \n",
      "  \n",
      "Marathi \n",
      " \n",
      " \n",
      "  \n",
      "Hindi \n",
      " \n",
      "  \n",
      "Websites, Portfolios, \n",
      "Profiles \n",
      "  \n",
      "• https://www.linkedin.com/in/sa\n",
      "gar-parab-23290a1a8/  \n",
      "  \n",
      "EDUCATION \n",
      "  \n",
      "  2011-04   M.Com from University of Mumbai   \n",
      "  2008-03   B.Com from University of Mumbai   \n",
      "  \n",
      "PROJECT EXPERIENCE\n",
      "\n",
      "expertise in advanced forecasting techniques.  \n",
      "• Deployed the application using Flask and Docker for \n",
      "scalable and efficient access.  \n",
      "  \n",
      "WEBSITES, PORTFOLIOS, PROFILES \n",
      "  \n",
      "https://www.linkedin.com/in/sagar -parab-23290a1a8/  \n",
      ". #HRJ#1dc3 0251-fb3f-4fb0-8990-f88b2a51 55a2#\n",
      "\n",
      "experience in machine learning, deep learning, and Gen -AI model deployment. \n",
      "Proven track record of delivering significant business impact using advanced Excel, \n",
      "VBA, Power BI, and Power Query. Eager to contribute to innovative, AI -driven \n",
      "solutions in a forward -thinking organization.  \n",
      "  \n",
      "PROFESSIONAL EXPERIENCE \n",
      "  \n",
      "  2022-03 - \n",
      "2024-09 \n",
      "  Senior Analyst  \n",
      "Accenture Services   \n",
      "• Designed and built comprehensive dashboards in Power \n",
      "BI, providing critical business insights.\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "messages =  graph.invoke({\"messages\":\"What is qualification of Sagar and what is 2 * 9\"})\n",
    "\n",
    "for message in messages[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dd3c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2419983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599cb6af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979056b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b610e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c25db9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AgenticAI_03",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
